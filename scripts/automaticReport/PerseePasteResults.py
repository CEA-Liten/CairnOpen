# -*- coding: utf-8 -*-
"""
@author: ARuby - 20/02/2020
"""
from typing import List

import os
import csv
import glob
import pandas as pd
import re

import Utilities as ut
import json

from lxml import etree
import copy

def PasteSortieFromGraphProperties(projectsPath: str, TestCase: str, scenarioList: List[str], ConfigCase: str, graphProperties: List[str], results_files = "_Results.csv", list_order=[]) -> object:
    """
    Finds all the PERSEE _Sortie.csv file containings time series of an optimization.
    Parses the graphProperties to find the variables of interest to be plotted

    The output DataFrame is written at "./projectsPath/SUMUP_TS_{ConfigCase}.csv"
    in the following format, with variables written as rows:

                ConfigCase                      Variable                    0       1
    0  T2bis_A6_Curt_GT...                     Data.Time  2004-12-31 02:00:00     ...
    1  T2bis_A6_Curt_GT...    Converter11kV_690V.PowerIn                    0     ...   
    2  T2bis_A6_Curt_GT...   Converter11kV_690V.PowerOut                    0     ...   
    3  T2bis_A6_Curt_GT...     Converter11kV_6kV.PowerIn                    0     ...  
    ...

    Args:
        projectsPath : str
            path of the project.
        TestCase : str
            Beginig name of the case name (scenario).
        graphProperties : (list[str])
            list of xml files to identify variables
        perseeGui : bool
            if true, the study has been generated by perseeGui (else, by Pegase). (Different gestion of time in the output TS file)
    Returns:
        DataFrame: the list of all the relevant dataseries of all the cases
    """
    variables = set()
    for file in graphProperties:
        if file != "":
            tree = etree.parse(projectsPath+ut.getOSsep()+file)
            variables = variables.union(set([b.text.strip() for b in tree.findall('.//variable')]) \
                .union(set([e.strip() for b in tree.findall('.//plotted_variables') for e in b.text.split(',')]))
            )
    variables = list(variables)
    print("====variables====",variables)
    
    PasteFile=os.path.join(projectsPath, "SUMUP_TS_"+ConfigCase+".csv")

    all_filenames = []
    strs=projectsPath+ut.getOSsep()+TestCase+'*'+ut.getOSsep()+'*'+'TNR'+'*'+ut.getOSsep()+'*_sortie.csv'
    all_filenames = [i for i in glob.glob(strs)]
    if all_filenames == []:  # when using PerseeGUI
        if len(scenarioList) < 2:
            scenario = ConfigCase
            if len(scenarioList) == 1:
                scenario = scenario + '_' + scenarioList[0]
            #check inside all folders starts with scenario
            strs=projectsPath + ut.getOSsep() + scenario+'*' + ut.getOSsep() + '*'+results_files
            all_filenames = [i for i in glob.glob(strs)]
        else:#multiple scenario names are given
            for name in scenarioList:
                scenario = ConfigCase + '_' + name
                #check only exact scenario folder
                strs=projectsPath + ut.getOSsep() + scenario + ut.getOSsep() + '*'+results_files
                all_filenames.extend([i for i in glob.glob(strs)])
        print("Reading rollinghorizon results :", all_filenames)

    df = pd.DataFrame()

    #re-order all_filenames //list list_report is not needed here
    list_report = [report.split(ut.getOSsep())[-2] for report in all_filenames]
    list_report, all_filenames = reOrderBasedOnList(list_report, all_filenames, list_order)

    #look for study_dataseries file
    start_time = ""
    try:
        dataseriesfile = projectsPath+ut.getOSsep()+TestCase+"_dataseries.csv"
        try:
            input_dataseries = pd.read_csv(dataseriesfile, sep=';', encoding = "utf-8")
        except UnicodeDecodeError:
            input_dataseries = pd.read_csv(dataseriesfile, sep=';', encoding = "ISO-8859-1")
        start_time = input_dataseries['Time'][0]
    except:
        print("Error while trying to get the start time from the data series file!")
        print("The name of the expected time series file is: ", dataseriesfile)
        print("Date-Time column should be the first column in the file, and its name should be 'Time'")
        print("Examples of start time valid formats are: 15/01/2012 00:00:01.99 and 2012-01-15 00:00:01.99")

    #Read result files
    for file in all_filenames:
        try:
            table = pd.read_csv(file, sep=';', skiprows=[1, 2], encoding = "utf-8")
        except UnicodeDecodeError:
            table = pd.read_csv(file, sep=';', skiprows=[1, 2], encoding = "ISO-8859-1")
        table.columns = table.columns.str.strip()
        if 'Data.Time' in table.columns:  # version pegase
            table = table[table.columns.intersection(variables + ['Data.Time'])]
            table['Data.Time'] = pd.to_datetime(table['Data.Time'], unit="s")
            case = file.split(ut.getOSsep())[-1]
            case = re.split('_(S|s)ortie', case)[0]
        else:  # version perseeGui
            table = table[table.columns.intersection(variables + ['Time'])]
            table['Data.Time'] = pd.to_datetime(table['Time'], unit="s")
            case = file.split(ut.getOSsep())[-2]
        #Add start_time as an offset to time values
        if start_time != "":
            try:
                delta_t = pd.Timedelta('0s')
                delta_t = pd.to_datetime(start_time, dayfirst=True) - pd.Timestamp(0, unit='s') #pd.Timestamp(1970,1,1) epoch  time 
                table['Data.Time'] = table['Data.Time'] + delta_t    
            except:
                print("Failed to add the start time: "+start_time+". Epoch time (1970-01-01) will be used")

        table.columns = pd.MultiIndex.from_product([[case], table.columns])
        table = table.T.reset_index()
        table.rename(columns={'level_0': 'ConfigCase', 'level_1':'Variable'}, inplace=True)

        df = pd.concat([df, table], axis=0, ignore_index=True)
        
    df.to_csv(PasteFile, sep=';', index=False, encoding="utf-8")
    print(f"SUMP_TS file written at {PasteFile}")
    return df
            
"""
Build report
"""

def PasteResults(projectsPath, TestCase, ConfigCase):
    """

    Parameters
    ----------
    projectsPath : str
        path of the project.
    TestCase : str
        Beginig name of the case name (scenario).
    ConfigCase : str
        name of the localisation (compares the cases on the same place).
    Returns
    -------
    Writhe files SUMUP_configCase in which plan file are gathered to be compared.

    """
    print("projectsPath  :",projectsPath) 
    
    PasteFile=projectsPath+ut.getOSsep()+"SUMUPALL"+ConfigCase+".csv"
    PasteFiletmp=projectsPath+ut.getOSsep()+"SUMUPALL.csv.tmp"

    strs=projectsPath+ut.getOSsep()+TestCase+'*'+ut.getOSsep()+'*'+ConfigCase+'*'+ut.getOSsep()+'SUMUP.csv'
    all_filenames = [i for i in glob.glob(strs)]
    
    liste_etudes=[]
    pays=""
    for f in range(len(all_filenames)):
        matchTest=re.search(TestCase+'\d_([a-zA-Z]+)\d_([a-zA-Z-_]+)',all_filenames[f]).group()
        print(f,matchTest)
        matchscenario=re.search('[Report_]([a-zA-Z]+)_(\d)+',all_filenames[f]).group()
        liste_etudes.append(matchTest)
        pays=matchscenario[1:]
    print("Reading results  :", all_filenames, " from ", strs)
    def enleveEspaces(x):
        return((x.replace(" ","")))
    def enleveTripleEspaces(x):
        return((x.replace("  "," ")))
    def enleveh(x):
        return(x.replace("#",''))
    def import_sumup(pays):
        loc=pays
        try:
            perf_data = [pd.read_csv(os.getcwd()+ut.getOSsep()+f+ut.getOSsep()+"Report_"+loc+ut.getOSsep()+"SUMUP.csv", sep=";",header=None, encoding='utf-8') 
                     for f in liste_etudes]
        except UnicodeDecodeError:
            perf_data = [pd.read_csv(os.getcwd()+ut.getOSsep()+f+ut.getOSsep()+"Report_"+loc+ut.getOSsep()+"SUMUP.csv", sep=";",header=None, encoding='ISO-8859-1') 
                     for f in liste_etudes]
        for i in range(len(perf_data)):
            #for j in range(len(perf_data[i][0])):
            perf_data[i][0] = perf_data[i][0].apply(enleveEspaces)
            perf_data[i][0] = perf_data[i][0].apply(enleveh)
            perf_data[i][1] = perf_data[i][1].apply(enleveTripleEspaces)
            perf_data[i]["Component.Variable"]=perf_data[i][0]+"."+perf_data[i][1]
            print(perf_data[i]["Component.Variable"])
            #perf_data[i]["Component.Variable"] = perf_data[i]["Component.Variable"].apply(enleveEspaces)
            print(perf_data[i]["Component.Variable"])
            perf_data[i]=perf_data[i].set_index("Component.Variable")
            perf_data[i]=perf_data[i].rename(columns={2:liste_etudes[i]+'_'+loc})
            del perf_data[i][0]
            del perf_data[i][1]
        return(perf_data)
    
    def sumup_file(pays):
        tab = import_sumup(pays)
        sumup=tab[0]
        for i in range(1,len(tab)):
            sumup=sumup.join(tab[i], how='outer')
        return(sumup)
    sumup_file(pays).to_csv(PasteFile, sep=';', encoding="utf-8")
    print(sumup_file(pays))
    return(PasteFile)
    

def PasteResultsMonoLoc(projectsPath, prefix, hist_plan, scenarioList=[], lines_to_delete = ["no data computed","save iteration"], file_out="SUMUPALL.csv", list_order=[]):
    """

    Parameters
    ----------
    projectsPath : str
        path of the project.
    scenarioList : list
        list of scenario names (when len < 2 the consider all scenario)
    prefix : str
        Beginning name of the case name (scenario).

    Returns
    -------
    Writhe files SUMUP_configCase in which plan file are gathered to be compared.

    """
    
    PasteFile=projectsPath+ut.getOSsep()+file_out

    list_report, all_filenames = generateReportList(projectsPath, prefix, scenarioList, hist_plan+'.csv')

    #Re-order reports in case of run sensitivity for result sumupall_sens.csv file
    list_report, all_filenames = reOrderBasedOnList(list_report, all_filenames, list_order)

    print("Reading results :", list_report ,'in', all_filenames)

    def enleveEspaces(x):
        return((x.replace(" ","")))
    def enleveTripleEspaces(x):
        return((x.replace("  "," ")))
    def enleveh(x):
        return(x.replace("#",''))
    def import_sumup():
        try:
            perf_data = [pd.read_csv(f, sep=";",header=None, usecols=[0,1,2], on_bad_lines='warn', encoding = "utf-8")  for f in all_filenames]
        except UnicodeDecodeError:
            perf_data = [pd.read_csv(f, sep=";",header=None, usecols=[0,1,2], on_bad_lines='warn', encoding = "ISO-8859-1")  for f in all_filenames]
        for i in range(len(perf_data)):
            perf_data[i].dropna(how="all", inplace=True)
            #for j in range(len(perf_data[i][0])):
            ndc =perf_data[i][1].str.contains("no data computed").index
            perf_data[i].drop(ndc)
            perf_data[i]["Component.Variable"]=perf_data[i][0]+"."+perf_data[i][1]
            for li in lines_to_delete:
                ndc = perf_data[i][perf_data[i]["Component.Variable"].str.contains(li)].index
            perf_data[i] = perf_data[i].drop(ndc)
            #perf_data[i]["Component.Variable"] = perf_data[i]["Component.Variable"].apply(enleveEspaces)
            perf_data[i]=perf_data[i].set_index("Component.Variable")
            perf_data[i]=perf_data[i].rename(columns={2:list_report[i]})
            del perf_data[i][0]
            del perf_data[i][1]
        return(perf_data)
    
    def sumup_file():
        tab = import_sumup()
        try: 
            sumup = tab[0]
            for i in range(1,len(tab)):
                sumup=sumup.join(tab[i], how='outer')
            return(sumup)
        except IndexError:
            print("Issue in sumup file, no run produced results")
    sumup=sumup_file()
    sumup.to_csv(PasteFile, sep=';', encoding="utf-8")
    return(sumup)

def checkParamInJson(filename: str,study_name:str):
    """store json parameters
    args:
    filename : address to the file .json
    """
    if os.path.isfile(filename):
        print("load json:", filename)
    with open(filename, 'r',encoding="utf-8") as study_file:
        parser = ut.loadStudyCompo(json.load(study_file))
    parametres = ["optionListJson","paramListJson","timeSeriesListJson", "envImpactsListJson"]
    dico = {"Case":[study_name]}
    for compo in ((parser["Components"])):
        for p in parametres:
            if p in compo:
                for k in compo[p]:
                    if k["value"]!="":
                        dico[compo["nodeName"]+"."+p+"."+k["key"]] = [k["value"]]
        if "nodePortsData" in compo:
            for k in compo["nodePortsData"]:
                for i in k["ports"]:
                    getParam(dico, i, compo["nodeName"] + "." + "nodePortsData " + "." + k["pos"] + "." + i["name"]+".","direction")
                    getParam(dico, i, compo["nodeName"] + "." + "nodePortsData " + "." + k["pos"] + "." + i["name"] + ".","type") #old studies
                    getParam(dico, i, compo["nodeName"] + "." + "nodePortsData " + "." + k["pos"] + "." + i["name"] + ".","carrier") #new studies
                    getParam(dico, i, compo["nodeName"] + "." + "nodePortsData " + "." + k["pos"] + "." + i["name"] + ".", "variable")
                    getParam(dico, i, compo["nodeName"] + "." + "nodePortsData " + "." + k["pos"] + "." + i["name"] + ".", "coeff")
                    getParam(dico, i, compo["nodeName"] + "." + "nodePortsData " + "." + k["pos"] + "." + i["name"] + ".", "offset")
    return dico

def getParam(dico_new,dico_search,string_base,param):
    if param in dico_search:
        dico_new[string_base+param]=[dico_search[param]]

def generateReportList(projectsPath, prefix, scenarioList, fileExt):
    all_filenames = []
    list_report = []
    if len(scenarioList) < 2:
        scenario = prefix 
        if len(scenarioList) == 1:
            scenario = scenario + '_' + scenarioList[0]
        #check inside all folders starts with scenario
        strs = projectsPath + ut.getOSsep() + scenario+'*' + ut.getOSsep() + '*'+fileExt
        print("Looking for a match to ", strs)
        if fileExt == '.json':
            all_filenames = list(set([i.replace("_self","") for i in glob.glob(strs)]))
        else:
            all_filenames = list(set([i for i in glob.glob(strs)]))

        for f in range(len(all_filenames)):
            print("searching in file: ", all_filenames[f])
            m = re.search(scenario + '([\d _a-zA-Z]+)', all_filenames[f])
            if type(m) is not type(None):
                matchTest = m.group()
                list_report.append(matchTest)
            elif len(scenarioList) == 1:
                m = re.search(scenario , all_filenames[f])
                if type(m) is not type(None):
                    list_report.append(scenario)
    else:#multiple scenario names are given
        for name in scenarioList:
            scenario = prefix + '_' + name
            #check only exact scenario folder
            strs = projectsPath + ut.getOSsep() + scenario + ut.getOSsep() + '*'+fileExt
            print("Looking for a match to ", strs)
            if fileExt == '.json':
                tmp_filenames = list(set([i.replace("_self","") for i in glob.glob(strs)]))
            else:
                tmp_filenames = list(set([i  for i in glob.glob(strs)]))
            if tmp_filenames:
                all_filenames.extend(tmp_filenames)
                list_report.append(scenario)
    return list_report, all_filenames

def reOrderBasedOnList(list_report, all_filenames, list_order):
    #Re-order list_report and all_filenames based on the order set in list_order
    ordered_reports, ordered_filenames = [], []
    other_reports = copy.deepcopy(list_report) 
    other_filenames = copy.deepcopy(all_filenames) 

    #Order matched file in the same order as appears in list_order
    for element in list_order: 
        for report in list_report:
            if report == "Report_s"+element or report == element:
                ordered_reports.append(report)
                i_report = list_report.index(report)
                ordered_filenames.append(all_filenames[i_report])

    #Obtain the remianing reports
    for report in list_report:
        if report in ordered_reports:
            i_report = other_reports.index(report)
            other_reports.pop(i_report)
            other_filenames.pop(i_report)

    # Use alphabetic order
    other_reports = sorted(other_reports)
    other_filenames = sorted(other_filenames)

    #merge lists
    ordered_reports.extend(other_reports)
    ordered_filenames.extend(other_filenames)

    if set(ordered_reports) == set(list_report) and set(ordered_filenames) == set(all_filenames):
        list_report = ordered_reports
        all_filenames = ordered_filenames

    return list_report, all_filenames

def getReportsOrder(projectsPath, tab_echantillonnage):
    list_order = []
    if tab_echantillonnage == "":
        #Use the same order as in sumupall_sens.csv if exists
        tab_param_file = os.path.join(projectsPath, "sumupall_sens.csv")
        if os.path.isfile(tab_param_file) and os.path.getsize(tab_param_file) > 0:
            try:
                tab_param = pd.read_csv(tab_param_file, sep=";", index_col=0, low_memory=False, encoding = "utf-8")
            except UnicodeDecodeError:
                tab_param = pd.read_csv(tab_param_file, sep=";", index_col=0, low_memory=False, encoding = "ISO-8859-1")
            print("Reports order has been taken from sumupall_sens.csv file")
            return tab_param.columns.tolist()

        #Use the same order as in SumupAll_s.csv if exists
        tab_param_file = os.path.join(projectsPath, "SumupAll_s.csv")
        if os.path.isfile(tab_param_file) and os.path.getsize(tab_param_file) > 0:
            try:
                tab_param = pd.read_csv(tab_param_file, sep=";", index_col=0, low_memory=False, encoding = "utf-8")
            except UnicodeDecodeError:
                tab_param = pd.read_csv(tab_param_file, sep=";", index_col=0, low_memory=False, encoding = "ISO-8859-1")
            print("Reports order has been taken from SumupAll_s.csv file")
            return tab_param.columns.tolist()

        #Use tab_echantillonnage.csv
        tab_echantillonnage = "tab_echantillonnage.csv"

    list_order = []
    tab_param_file = os.path.join(projectsPath, tab_echantillonnage)
    if os.path.isfile(tab_param_file) and os.path.getsize(tab_param_file) > 0:
        print("Reports order has been taken from " + tab_echantillonnage + " file")
        try:
            tab_param = pd.read_csv(tab_param_file, sep=";", index_col=0, low_memory=False, encoding = "utf-8")
        except UnicodeDecodeError:
            tab_param = pd.read_csv(tab_param_file, sep=";", index_col=0, low_memory=False, encoding = "ISO-8859-1")
        list_order = [str(i) for i in tab_param.index.values] 
        return list_order
    
    #Check for order_file.csv file
    tab_param_file = os.path.join(projectsPath, "order_file.csv")
    if os.path.isfile(tab_param_file) and os.path.getsize(tab_param_file) > 0:
        print("Reports order has been taken from order_file.csv file")
        try:
            tab_param = pd.read_csv(tab_param_file, sep=";", index_col=0, low_memory=False, encoding = "utf-8")
        except UnicodeDecodeError:
            tab_param = pd.read_csv(tab_param_file, sep=";", index_col=0, low_memory=False, encoding = "ISO-8859-1")
        list_order = [str(i) for i in tab_param.index.values] 

    return list_order



def gatherParamInJson(projectsPath, prefix, scenarioList=[], tab_echantillonnage=""):

    list_report, all_filenames = generateReportList(projectsPath, prefix, scenarioList, '.json')

    #Get reports order
    list_order = getReportsOrder(projectsPath, tab_echantillonnage)

    #Re-order reports
    list_report, all_filenames = reOrderBasedOnList(list_report, all_filenames, list_order)

    print("Reading json files  :", list_report, 'in', all_filenames)

    df_list = [pd.DataFrame.from_dict(checkParamInJson(filename, list_report[i])) for i,filename in enumerate(all_filenames)]

    result = pd.concat(df_list, ignore_index=True)
    return result, list_report

def compare_json(data: pd.DataFrame):
    datafiltre = data
    for c in data.columns:
        try:
            if (data[c] == data[c][0]).all():
                del datafiltre[c]
            else:
                continue
        except:
            #case when indicator names include square brackets interpreted as ... listsby all() method !
            ikeep=0
            for i in data[c]:
                if i != data[c][0]:
                    ikeep=1
            if ikeep==0:
                del datafiltre[c]
            else:
                continue
    return (datafiltre)

def getCplexInfo(projectsPath, prefix, scenarioList=[], tab_echantillonnage=""):

    list_report, all_filenames = generateReportList(projectsPath, prefix, scenarioList, 'optim.log')

    #Get reports order
    list_order = getReportsOrder(projectsPath, tab_echantillonnage)

    #Re-order reports
    list_report, all_filenames = reOrderBasedOnList(list_report, all_filenames, list_order)

    print("Reading cplex log files  :", list_report, 'in', all_filenames)

    dico = {'Case' : [], 'tmps_resolution':[], 'bugs':[], 'precision':[], "objectif":[]}
    for i,cplex_log in enumerate(all_filenames):
        with open(cplex_log) as f:
            content = f.readlines()
        dico['Case'].append(list_report[i])
        tmps_resol_list = []
        bugs = 0
        cas = 0
        precision = 0
        precision_list = []
        tmpsres = 0
        obj=0
        for c in content:
            if (c[0:25] == "Total (root+branch&cut) ="):
                tmps_resol_list.append(float(c[25:].split(sep="s")[0]))
                tmpsres += float(c[25:].split(sep="s")[0])
                cas += 1
                precision_list.append(precision)
            if (c[0:38] == "CPLEX Error  1217: No solution exists."):
                bugs += 1
                precision_list.append(100)
                tmps_resol_list.append(0)
            if ("%" in c):
                precision = c.split()[-1]
                obj = c.split()[-3]
        dico["tmps_resolution"].append(tmpsres)
        dico["bugs"].append(bugs)
        dico["precision"].append(precision)
        dico["objectif"].append(obj)
    df = pd.DataFrame.from_dict(dico)
    return(df)